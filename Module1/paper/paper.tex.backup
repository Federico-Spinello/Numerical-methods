\documentclass[10pt]{article}

% Pacchetti
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{physics}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[margin=2cm]{geometry}
\usepackage{multicol}

% Definizioni custom
\newcommand{\ising}{\text{Ising}}
\newcommand{\mc}{\text{MC}}
\DeclareMathOperator{\prob}{P}

% Info documento
\title{\Large\textbf{Studio Numerico della Transizione di Fase del Modello di Ising 2D mediante Algoritmo di Wolff}}

\author{Federico Spinello}

\date{\today}

\begin{document}

\maketitle

% Abstract a colonna singola
\begin{abstract}
\noindent
In questo lavoro vengono studiate numericamente la transizione di fase ferromagnetica del modello di Ising bidimensionale su reticolo quadrato mediante simulazioni Monte Carlo. Viene utilizzato l'algoritmo di Wolff per superare il fenomeno del critical slowing down che affligge gli algoritmi locali come Metropolis. Attraverso l'analisi di Finite Size Scaling per reticoli di dimensioni $L = 20, 30, 40, 50, 60, 80, 100, 120, 140, 160, 180, 200, 240$ e temperature nel range $T \in [2.0, 2.5]$ (100 punti, 60 punti per $L=240$ ), determiniamo la temperatura critica tramite il crossing del Binder cumulant $T_c = 2.269 \pm 0.006$ e l'esponente critico tramite scaling della suscettività $\gamma/\nu = 1.74 \pm 0.00$, in eccellente accordo con i valori esatti noti dalla soluzione di Onsager ($T_c = 2.269$ e $\gamma/\nu = 1.75$). Il data collapse conferma ulteriormente la validità della teoria FSS. I risultati dimostrano l'efficacia dell'algoritmo di Wolff e l'accuratezza dei metodi Monte Carlo per lo studio delle transizioni di fase.
\end{abstract}

% Inizio doppia colonna
\begin{multicols}{2}

\section{Introduzione}

Il modello di Ising rappresenta uno dei paradigmi fondamentali della meccanica statistica. Nonostante la sua semplicità—variabili di spin discrete su reticolo con interazioni tra primi vicini—cattura la fisica delle transizioni di fase di secondo ordine.

Nel 1944, Lars Onsager ottenne la soluzione esatta del modello di Ising bidimensionale, determinando la temperatura critica
\begin{equation}
    T_c = \frac{2J}{k_B \ln(1 + \sqrt{2})} \simeq 2.269
\end{equation}

Un problema critico è il \emph{critical slowing down}: vicino a $T_c$, algoritmi locali come Metropolis diventano inefficienti. L'algoritmo di Wolff (1989) risolve questo problema usando cluster updates.

\subsection{Obiettivi}

Questo lavoro si propone di:
\begin{enumerate}
    \item Determinare $T_c$ tramite picchi di suscettività, picchi del calore specifico e Binder cumulant
    \item Stimare l'esponente critico $\gamma/\nu$
    \item Validare il Finite Size Scaling via data collapse
\end{enumerate}

\section{Il Modello di Ising 2D}

\subsection{Definizione}

Il modello è definito su un reticolo quadrato $\Lambda$ di dimensione $L \times L$ con $N = L^2$ siti. A ciascun sito $i \in \Lambda$ è associata una variabile di spin:
\begin{equation}
    s_i \in \{-1, +1\}
\end{equation}

L'Hamiltoniana ferromagnetica è:
\begin{equation}
    \mathcal{H} = -J \sum_{\langle i,j \rangle} s_i s_j
\end{equation}
dove $J > 0$ (poniamo $J=1$) e $\langle i,j \rangle$ indica coppie di primi vicini. Usiamo condizioni periodiche al contorno (PBC).

\subsection{Transizione di Fase}

Il sistema presenta una transizione di fase di secondo ordine alla temperatura critica $T_c$:

\begin{itemize}
    \item \textbf{Fase ferromagnetica} ($T < T_c$): Magnetizzazione spontanea $m \neq 0$, ordine a lungo range.
    \item \textbf{Fase paramagnetica} ($T > T_c$): Magnetizzazione $m = 0$, spin scorrelati.
\end{itemize}

A $T_c$ emerge un comportamento critico caratterizzato dalla divergenza della \emph{lunghezza di correlazione} $\xi(T)$. La lunghezza di correlazione è la distanza caratteristica oltre la quale gli spin perdono memoria dell'orientazione reciproca. Più precisamente, la funzione di correlazione spin-spin decade esponenzialmente:
\begin{equation}
    \langle s_i s_j \rangle \sim e^{-|r_i - r_j|/\xi(T)}
\end{equation}
dove $|r_i - r_j|$ è la distanza tra i siti $i$ e $j$.

Vicino alla temperatura critica, $\xi$ diverge secondo la legge di potenza:
\begin{equation}
    \xi(T) \sim |T - T_c|^{-\nu}
\end{equation}
con $\nu = 1$ per Ising 2D. Nella fase ferromagnetica ($T < T_c$), $\xi$ rappresenta la dimensione tipica dei domini ordinati; nella fase paramagnetica ($T > T_c$), misura la distanza su cui gli spin rimangono correlati prima di disordinarsi. Esattamente a $T_c$, $\xi = \infty$: il sistema presenta correlazioni a lungo raggio e strutture a tutte le scale (invarianza di scala).

\subsection{Esponenti Critici}

Il comportamento vicino a $T_c$ è governato da:
\begin{align}
    m(T) &\sim (T_c - T)^\beta, && \beta = 1/8 \\
    \chi(T) &\sim |T - T_c|^{-\gamma}, && \gamma = 7/4 \\
    \xi(T) &\sim |T - T_c|^{-\nu}, && \nu = 1 \\
    C(T) &\sim -\ln|T - T_c|, && \alpha = 0
\end{align}

Questi valori sono esatti (Onsager) e universali: dipendono solo da dimensionalità ($d=2$), simmetria ($\mathbb{Z}_2$) e range delle interazioni.

\subsection{Finite Size Scaling}

Su un reticolo finito, $\xi$ non può superare $L$. La teoria di Finite Size Scaling (FSS) descrive come le osservabili termodinamiche si comportano in sistemi di dimensione finita vicino alla transizione di fase.

Consideriamo un'osservabile generica $O(T,L)$ che dipende sia dalla temperatura $T$ che dalla dimensione del sistema $L$ (ad esempio, magnetizzazione, suscettività, energia, etc.). La teoria FSS prevede che questa osservabile possa essere fattorizzata come:
\begin{equation}
    O(T,L) = L^{\lambda/\nu} \tilde{O}\left(L^{1/\nu}(T - T_c)\right)
\end{equation}
dove:
\begin{itemize}
    \item $O(T,L)$ è il valore misurato dell'osservabile su un reticolo finito $L \times L$
    \item $L^{\lambda/\nu}$ è un fattore di scala che dipende dall'esponente critico $\lambda$ dell'osservabile e dall'esponente $\nu$ della lunghezza di correlazione
    \item $\tilde{O}(x)$ è una \emph{funzione di scaling universale}, indipendente da $L$ e $T$ separatamente, che dipende solo dalla combinazione adimensionale $x = L^{1/\nu}(T - T_c)$
\end{itemize}

La potenza di questa relazione è che \emph{tutte} le curve $O(T,L)$ per diversi valori di $L$, se riscalate opportunamente, collassano su un'unica curva universale $\tilde{O}(x)$. Questo è il fenomeno del \emph{data collapse}.

Per la suscettività ($\lambda = \gamma$):
\begin{equation}
    \chi(T, L) = L^{\gamma/\nu} \tilde{\chi}\left(L^{1/\nu}(T - T_c)\right)
\end{equation}

Conseguenze:
\begin{itemize}
    \item Picco a $T_{\max}(L) \approx T_c + \text{const} \cdot L^{-1/\nu}$
    \item Altezza: $\chi_{\max}(L) \sim L^{\gamma/\nu}$
    \item Data collapse: plottando $\chi/L^{\gamma/\nu}$ vs $L^{1/\nu}(T-T_c)$, tutte le curve collassano
\end{itemize}

\subsection{Binder Cumulant}

Il Binder cumulant è definito come:
\begin{equation}
    U_L = \frac{\langle m^4 \rangle}{\langle m^2 \rangle^2}
\end{equation}

Proprietà:
\begin{itemize}
    \item $T \ll T_c$: $U_L \to 1$ (fase ordinata)
    \item $T \gg T_c$: $U_L \to 3$ (fase disordinata, fluttuazioni gaussiane)
    \item $T = T_c$: $U_L = U^* \approx 1.17$ (universale per Ising 2D)
\end{itemize}

Le curve $U_L(T)$ per diversi $L$ si intersecano a $T_c$ (\emph{crossing point}).

\section{Metodi Monte Carlo}

\subsection{Il Problema Computazionale}

In meccanica statistica, il valore medio di un'osservabile nell'ensemble canonico richiede:
\begin{equation}
    \langle O \rangle = \frac{1}{Z} \sum_{\{s\}} O(\{s\}) e^{-\beta \mathcal{H}(\{s\})}
\end{equation}
dove $Z = \sum_{\{s\}} e^{-\beta \mathcal{H}}$ è la funzione di partizione.

Per $N = L^2$ spin, esistono $2^N$ configurazioni possibili. Esempi numerici:
\begin{itemize}
    \item $L = 10$: $2^{100} \approx 10^{30}$ configurazioni
    \item $L = 20$: $2^{400} \approx 10^{120}$ configurazioni (più atomi nell'universo!)
    \item $L = 100$: $2^{10000} \approx 10^{3010}$ configurazioni 
\end{itemize}

Il calcolo esatto è \textbf{impossibile} per sistemi realistici.

\subsection{Importance Sampling}

La soluzione Monte Carlo: invece di sommare su \emph{tutte} le configurazioni, si campionano solo quelle importanti secondo la distribuzione di Boltzmann:
\begin{equation}
    P(\{s\}) = \frac{1}{Z} e^{-\beta \mathcal{H}(\{s\})}
\end{equation}

Con $M$ configurazioni campionate correttamente:
\begin{equation}
    \langle O \rangle \approx \frac{1}{M} \sum_{i=1}^M O(\{s^{(i)}\})
\end{equation}

Con $M \sim 10^5$ misure si ottiene precisione statistica eccellente, anche se $M \ll 2^N$!

\subsection{Algoritmo di Metropolis}

L'algoritmo di Metropolis (1953) genera configurazioni secondo $P(\{s\})$ usando una catena di Markov:

\begin{enumerate}
    \item Scegli un sito random $(i,j)$
    \item Proponi flip: $s_{ij} \to -s_{ij}$
    \item Calcola $\Delta E = E_{\text{nuova}} - E_{\text{vecchia}}$
    \item Accetta con probabilità:
    \begin{equation}
        P_{\text{acc}} = \min\{1, e^{-\beta \Delta E}\}
    \end{equation}
\end{enumerate}

Questo garantisce il \emph{detailed balance} e convergenza a $P(\{s\})$.

\subsection{Critical Slowing Down}

Vicino a $T_c$, Metropolis diventa estremamente inefficiente. Questo perché:

\begin{itemize}
    \item La lunghezza di correlazione diverge: $\xi \sim |T - T_c|^{-\nu}$
    \item Si formano cluster di spin correlati di dimensione $\sim \xi$
    \item Metropolis flippa \emph{un singolo spin} per volta
    \item Per decorrelazione servono $\tau \sim \xi^z$ step, con $z \approx 2$ (dinamica locale)
\end{itemize}

Risultato: a $T_c$ su reticolo $L \times L$:
\begin{equation}
    \tau_{\text{Metropolis}} \sim L^2
\end{equation}

Per $L = 100$: servono $\sim 10000$ volte più step che per $L = 10$!

\subsection{Algoritmo di Wolff}

Wolff (1989) risolve il critical slowing down usando \emph{cluster updates}: invece di flippare singoli spin, flippa interi cluster di spin correlati.

\subsubsection{Costruzione del Cluster}

\begin{enumerate}
    \item Scegli seed $s_0$ casuale
    \item Inizializza cluster: $C = \{s_0\}$, $\text{occupati} = \{s_0\}$
    \item Calcola probabilità: $P_{\text{add}} = 1 - e^{-2\beta}$
    \item Per ogni sito $s \in C$:
    \begin{itemize}
        \item Esamina i 4 primi vicini
        \item Se il vicino ha \emph{stesso spin} e \emph{non è occupato}: aggiungi a C  con prob.  $P_{add}$
    \end{itemize}
    \item Ripeti finché nessun nuovo sito viene aggiunto
    \item \textbf{Flippa tutto il cluster} $C$ simultaneamente
\end{enumerate}

\subsubsection{Detailed Balance}

Wolff dimostrò che questa procedura rispetta il detailed balance con $P_{\text{add}} = 1 - e^{-2\beta}$. La probabilità di costruire un cluster $C$ e il suo complemento $\bar{C}$ sono identiche.

\subsubsection{Efficienza}

Il vantaggio cruciale: a $T_c$, dove esistono cluster naturali di dimensione $\sim L$, Wolff li flippa in \emph{un singolo step}!

Tempo di autocorrelazione:
\begin{align}
    \tau_{\text{Metropolis}} &\sim L^{z}, && z \approx 2 \\
    \tau_{\text{Wolff}} &\sim L^{z_W}, && z_W \approx 0.25
\end{align}

\textbf{Speed-up per $L = 100$}:
\begin{equation}
    \frac{\tau_{\text{Metropolis}}}{\tau_{\text{Wolff}}} \approx \frac{100^2}{100^{0.25}} \approx \frac{10000}{3.16} \approx 3160
\end{equation}

Wolff è \textbf{oltre 3000 volte più veloce} vicino a $T_c$!

\subsection{Implementazione Non-Ricorsiva}

Per evitare stack overflow su reticoli grandi, implementiamo Wolff in modo iterativo usando una coda di siti da esplorare (\texttt{pointtoocc}). Questo permette di costruire cluster fino a $L \sim 1000$ senza problemi di memoria.

\section{Dettagli Simulazione}

\subsection{Parametri}

\begin{table}[H]
\centering
\small
\caption{Parametri delle simulazioni}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Parametro} & \textbf{Valore} \\
\midrule
Dimensioni $L$ & 20, 30, 40, 50, 60, 80, 100,\\
                & 120, 140, 160, 180, 200, 240 \\
Temperature $T$ & 2.0--2.5 (100 punti, \\
                &ad eccezzione di $L=240$, con 60 punti) \\
Termalizzazione & 10000 cluster updates \\
Misure & 100000 cluster updates \\
Condizioni contorno & Periodiche (PBC) \\
Unità & $J = k_B = 1$ \\
RNG & PCG32 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Osservabili}
Le osservabili che vengono analizzate sono le seguenti:
\paragraph{Magnetizzazione per spin (valore assoluto medio)}
La magnetizzazione istantanea è definita come:
\begin{equation}
    m = \frac{1}{N} \sum_{i=1}^N s_i
\end{equation}
Per studiare la transizione di fase, misuriamo il valore assoluto medio:
\begin{equation}
    \langle |m| \rangle
\end{equation}
che è diverso da zero anche senza campo magnetico esterno nella fase ferromagnetica, grazie alla rottura spontanea di simmetria $\mathbb{Z}_2$.
\paragraph{Energia per spin}
\begin{equation}
    E = -\frac{1}{N} \sum_{\langle i,j \rangle} s_i s_j
\end{equation}

\paragraph{Suscettività}
\begin{equation}
    \chi = \beta N \left(\langle m^2 \rangle - \langle |m| \rangle^2\right)
\end{equation}

\paragraph{Calore specifico}
Il calore specifico è definito come la derivata dell'energia rispetto alla temperatura:
\begin{equation}
    C = \frac{\partial \langle E \rangle}{\partial T}
\end{equation}
e viene calcolato tramite il teorema di fluttuazione-dissipazione:
\begin{equation}
    C = \beta^2 N \left(\langle E^2 \rangle - \langle E \rangle^2\right)
\end{equation}

\subsection{Termalizzazione e Analisi degli Errori}

\paragraph{Scelta del periodo di termalizzazione}
Prima di raccogliere misure statisticamente significative, il sistema deve raggiungere l'equilibrio termico. Il numero di step di termalizzazione (10000 cluster updates) è stato scelto in modo conservativo osservando l'evoluzione temporale dell'energia e della magnetizzazione per diversi valori di $L$ e $T$.

In particolare, vicino a $T_c$ dove il sistema termalizza più lentamente a causa del critical slowing down, si è verificato che dopo 10000 cluster updates di Wolff le osservabili raggiungono un regime stazionario e le correlazioni con la configurazione iniziale (completamente ordinata o completamente disordinata) sono completamente perse.

\paragraph{Autocorrelazione e errori statistici}
L'algoritmo di Wolff riduce drasticamente il tempo di autocorrelazione $\tau$ rispetto a Metropolis. Tuttavia, misure consecutive rimangono correlate. Per stimare correttamente gli errori statistici, si dovrebbe in linea di principio utilizzare il metodo del blocking o calcolare esplicitamente $\tau$ per ogni osservabile.

In questo lavoro, gli errori riportati per le quantità derivate da fit (come $T_c$ e $\gamma/\nu$) sono gli errori statistici forniti da \texttt{scipy.optimize.curve\_fit}, che assumono misure indipendenti. Gli errori sui singoli punti dati (energia, magnetizzazione, ecc.) non sono mostrati esplicitamente nei grafici per chiarezza visiva, ma sono tipicamente molto piccoli grazie all'elevato numero di misure (100000 cluster updates per punto).

\paragraph{Finite Size Scaling degli errori}
Gli errori sui parametri critici estratti (in particolare $T_c$) riflettono principalmente gli effetti di finite size e la sensibilità del metodo di estrazione, piuttosto che fluttuazioni statistiche pure. L'incertezza sul Binder crossing, ad esempio, deriva dalla dispersione residua tra le curve di diversi $L$ nella regione di crossing.

\subsection{Implementazione}

\subsection{Architettura del Software}
Particolare importanza è stata data alla progettazione dell'architettura del software. Questa è modulare, separando nettamente le fasi di simulazione (C), analisi (Python) e presentazione (LaTeX). Questa separazione segue il principio di \emph{separation of concerns} e garantisce riproducibilità e manutenibilità.

\paragraph{Design Pattern: Pipeline Multi-Stage}

L'intero workflow è organizzato come una pipeline in 4 stage:

\begin{enumerate}
    \item \textbf{Compilazione}: Codice C compilato con ottimizzazioni \texttt{-O3 -march=native}
    \item \textbf{Simulazione}: Esecuzione Monte Carlo su griglia
    \item \textbf{Analisi}: Post-processing Python con calcolo esponenti e grafici
    \item \textbf{Presentazione}: Aggiornamento automatico paper LaTeX con risultati
\end{enumerate}

Ogni stage comunica tramite \emph{file di dati}, permettendo riesecuzioni parziali e debugging indipendente.

\paragraph{Modularizzazione del Codice C}

Il simulatore C è diviso in moduli funzionali:

\begin{itemize}
    \item \textbf{\texttt{main.c}}: Orchestratore batch
    \begin{itemize}
        \item Parsing \texttt{params.txt} (dimensioni L, range temperature)
        \item Costruzione griglia $(L \times T)$ con spaziatura uniforme
        \item Loop esterno su tutte le combinazioni (L, T)
        \item Invoca \texttt{run\_ising\_simulation()} per ogni punto
    \end{itemize}

    \item \textbf{\texttt{ising\_wolff.c/h}}: Core simulazione
    \begin{itemize}
        \item Implementazione algoritmo di Wolff (cluster updates)
        \item Loop Monte Carlo (termalizzazione + misure)
        \item Costruzione cluster non ricorsiva
        \item Calcolo osservabili (m, E, $\chi$, C, Binder)
    \end{itemize}

    \item \textbf{\texttt{geometry.c/h}}: Costruzione griglia
    \begin{itemize}
        \item Condizioni contorno periodiche (PBC)
        \item Calcolo indici vicini (N, S, E, W)
        \item Mapping 2D $\to$ 1D: $(i,j) \to i \cdot L + j$
    \end{itemize}

    \item \textbf{\texttt{random.c/h}}: Wrapper RNG
    \begin{itemize}
        \item Interfaccia uniforme per PCG32
        \item Inizializzazione con seed da tempo
    \end{itemize}

    \item \textbf{\texttt{pcg32min.c/h}}: Generatore PCG32
    \begin{itemize}
        \item Implementazione algoritmo di generazione numeri random
    \end{itemize}
\end{itemize}

Questa architettura separa l'orchestrazione batch (main.c) dalla logica di simulazione (ising\_wolff.c), rendendo il codice più manutenibile e permettendo futuro riuso del modulo Wolff in altri contesti.

\paragraph{Automazione con Makefile}

Il \texttt{Makefile} implementa target dichiarativi per l'intera pipeline:

\begin{itemize}
    \item \texttt{make all}: Compilazione con dipendenze automatiche
    \item \texttt{make run}: Esegue la simulazione
    \item \texttt{make analyze}: Esegue \texttt{analyze.py} $\to$ genera i risultati, salvati in \texttt{risultati.json}
    \item \texttt{make update}: Aggiorna valori nel paper da \texttt{risultati.json}
    \item \texttt{make paper}: \texttt{update} + compilazione LaTeX
    \item \texttt{make fullpaper}: Pipeline completa end-to-end
\end{itemize}

\paragraph{Configurazione Parametrica}

Per una maggiore manutenbilità e chiarezza i parametri iniziali si trovano in \texttt{params.txt}:
\begin{itemize}
    \item \texttt{L\_VALUES}: Lista dimensioni reticolo
    \item \texttt{T\_MIN}, \texttt{T\_MAX}, \texttt{N\_TEMPS}: Griglia temperature
    \item \texttt{THERMALIZATION}, \texttt{MEASUREMENTS}: Numero update MC
    \item \texttt{DATA\_DIR}: Directory output
\end{itemize}

L'eseguibile \texttt{bin/ising\_simulation} legge \texttt{params.txt} all'avvio, costruisce la griglia $(L \times T)$ e lancia tutte le simulazioni.

\paragraph{Post-Processing Python}

\texttt{analyze.py} implementa l'intera analisi dei dati:
\begin{itemize}
    \item Caricamento dati dai file .dat
    \item Calcolo Binder crossing
    \item Fit power-law $\chi_{\max} \sim L^{\gamma/\nu}$ con \texttt{scipy.curve\_fit}
    \item Data collapse FSS con riscalamento
    \item Generazione 6 grafici (magnetizzazione, suscettività, scaling, collapse, Binder, energia/calore)
    \item Export risultati in \texttt{risultati.json} e \texttt{risultati.txt}
\end{itemize}

\paragraph{Aggiornamento Automatico Paper}

\texttt{update\_paper.py} modifica \texttt{paper.tex} automaticamente:
\begin{itemize}
    \item Legge valori da \texttt{risultati.json}
    \item Sostituisce pattern LaTeX: \texttt{T\_c = X \textbackslash pm Y}, \texttt{\textbackslash gamma/\textbackslash nu = ...}, \texttt{R\textasciicircum 2 = ...}
    \item Crea backup automatico (\texttt{paper.tex.backup})
    \item Garantisce consistenza tra simulazioni e documento
\end{itemize}

Questo approccio elimina errori di trascrizione manuale e permette rigenerazione completa del paper con \texttt{make fullpaper}.

\section{Risultati}

\subsection{Magnetizzazione}

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{../plots/magnetization.png}
    \caption{Magnetizzazione vs temperatura. La transizione diventa più ripida con $L$.}
    \label{fig:mag}
\end{figure}

La Figura~\ref{fig:mag} mostra:
\begin{itemize}
    \item $T < T_c$: magnetizzazione spontanea (fase ordinata)
    \item $T \approx T_c$: transizione rapida
    \item $T > T_c$: $m \to 0$ (fase disordinata)
    \item La transizione diventa più ripida all'aumentare di $L$ (effetto finite size)
\end{itemize}

\subsection{Suscettività}

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{../plots/susceptibility.png}
    \caption{Suscettività vs temperatura. Media dei picchi marcato a $T_c$.}
    \label{fig:chi}
\end{figure}

I picchi di $\chi$ (Fig.~\ref{fig:chi}) divergono con $L$ secondo $\chi_{\max} \sim L^{\gamma/\nu}$. La posizione dei picchi si sposta con $L$ secondo la legge di Finite Size Scaling:
\begin{equation}
    T_{\max}^{(\chi)}(L) = T_c + a \cdot L^{-1/\nu}
\end{equation}
dove $\nu = 1$ per Ising 2D. Fittando questa relazione ai dati si ottiene:
\begin{equation}
    T_c^{(\chi)} = 2.270 \pm 0.001, \quad a = 1.92 \pm 0.05
\end{equation}
Questo metodo è più accurato della semplice media dei picchi, poiché tiene conto esplicitamente degli effetti di finite size. Tuttavia, rimane meno preciso del metodo del Binder crossing.

\subsection{Scaling di $\chi_{\max}$}

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{../plots/chi_scaling.png}
    \caption{Scaling log-log di $\chi_{\max}$ vs $L$. Fit: $\gamma/\nu = 1.739 \pm 0.004$.}
    \label{fig:scaling}
\end{figure}

La Figura~\ref{fig:scaling} mostra $\chi_{\max} \sim L^{\gamma/\nu}$ con fit usando \texttt{scipy.optimize.curve\_fit}:
\begin{equation}
    \gamma/\nu = 1.739 \pm 0.004, \quad R^2 = 0.999974
\end{equation}
Il valore teorico è $\gamma/\nu = 1.75$. La differenza percentuale è dello 0.6\%, corrispondente a circa 2.75 deviazioni standard. Questa discrepanza, seppur piccola in termini relativi, suggerisce la presenza di correzioni finite-size residue non catturate dal semplice scaling $\chi_{\max} \sim L^{\gamma/\nu}$, che vale rigorosamente solo nel limite $L \to \infty$

\subsection{Data Collapse}

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{../plots/fss_collapse.png}
    \caption{Finite Size Scaling: data collapse della suscettività. La curva riscalata rappresenta la funzione di scaling universale $\tilde{\chi}(x)$.}
    \label{fig:fss}
\end{figure}
Il collasso delle curve (Fig.~\ref{fig:fss}) valida la teoria FSS usando $T_c = 2.269$ ottenuto dal crossing del Binder cumulant. La figura mostra la funzione di scaling universale $\tilde{\chi}(x)$ ottenuta plottando $\chi/L^{\gamma/\nu}$ vs $x = L^{1/\nu}(T - T_c)$. Il fatto che tutte le curve per diversi valori di $L$ collassino su un'unica curva conferma l'invarianza di scala della funzione universale e l'accuratezza dei parametri critici utilizzati ($T_c$, $\gamma/\nu$).

\subsection{Binder Cumulant}

Il Binder cumulant (o quarto cumulante) è una quantità adimensionale particolarmente utile per determinare la temperatura critica in sistemi finiti. È definito come:
\begin{equation}
    U_L = \frac{\langle m^4 \rangle}{\langle m^2 \rangle^2}
\end{equation}

Questa quantità ha proprietà notevoli che la rendono ideale per l'analisi FSS:

\paragraph{Comportamento limite:}
\begin{itemize}
    \item \textbf{Fase ordinata} ($T \ll T_c$): La magnetizzazione è quasi costante, $m \approx m_0$. Quindi $\langle m^4 \rangle \approx \langle m^2 \rangle^2$ e $U_L \to 1$.
    \item \textbf{Fase disordinata} ($T \gg T_c$): Le fluttuazioni sono gaussiane. Per una distribuzione gaussiana, $\langle m^4 \rangle = 3\langle m^2 \rangle^2$, quindi $U_L \to 3$.
\end{itemize}

\paragraph{Proprietà di scaling:}
Il Binder cumulant ha la caratteristica di essere \emph{adimensionale} e quindi, secondo FSS:
\begin{equation}
    U_L(T) = \tilde{U}\left(L^{1/\nu}(T - T_c)\right)
\end{equation}
Non ha il prefattore $L^{\lambda/\nu}$. Questo significa che le curve $U_L(T)$ per diversi $L$ si \emph{intersecano esattamente} a $T_c$, fornendo un metodo preciso e robusto per determinare la temperatura critica.

\paragraph{Metodo di determinazione del crossing point}
Il punto di crossing è stato determinato minimizzando la dispersione (deviazione standard) tra i valori di $U_L(T)$ per tutti gli $L$ considerati, in funzione della temperatura. In altre parole, si cerca la temperatura $T^*$ alla quale la varianza:
\begin{equation}
    \sigma^2(T) = \frac{1}{N_L} \sum_{i=1}^{N_L} \left[U_{L_i}(T) - \bar{U}(T)\right]^2
\end{equation}
è minima, dove $N_L$ è il numero di dimensioni simulate e $\bar{U}(T)$ è la media di $U_L(T)$ su tutti gli $L$.

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{../plots/binder_dispersion.png}
    \caption{Dispersione dei valori di $U_L(T)$ in funzione di $T$. Il minimo indica il crossing point e fornisce una stima di $T_c$.}
    \label{fig:binder_disp}
\end{figure}

La Figura~\ref{fig:binder_disp} mostra la dispersione $\sigma(T)$ in funzione della temperatura. Il minimo si trova a $T_c \approx 2.269$.

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{../plots/binder_cumulant.png}
    \caption{Binder cumulant vs temperatura. Il crossing point delle curve per diversi $L$ determina $T_c$.}
    \label{fig:binder}
\end{figure}

La Figura~\ref{fig:binder} mostra le curve $U_L(T)$ per diversi valori di $L$. Il punto di incrocio (\emph{crossing point}) determina:
\begin{equation}
    T_c = 2.269 \pm 0.006
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{../plots/binder_zoom.png}
    \caption{Zoom del Binder crossing. Le curve non si incrociano in un unico punto preciso, ma in una piccola regione. Questo riflette gli effetti di finite-size residui.}
    \label{fig:binder_zoom}
\end{figure}

La Figura~\ref{fig:binder_zoom} mostra uno zoom della regione di crossing. È importante notare che le curve \emph{non} si incrociano esattamente in un unico punto: esiste una piccola regione di crossing dove le curve convergono ma non coincidono perfettamente. Questo riflette il fatto che, contrariamente a quanto talvolta affermato, il crossing point \emph{non} è completamente insensibile agli effetti di finite-size. Tuttavia, questi effetti sono più deboli rispetto al metodo dei picchi di $\chi$ o $C$, rendendo il Binder crossing il metodo più accurato tra quelli considerati.

L'errore riportato ($\pm 0.006$) riflette l'ampiezza della regione di crossing ed è stimato come la semi-larghezza dell'intervallo di temperature dove $\sigma(T)$ rimane entro il 5\% del suo valore minimo. Il risultato è in eccellente accordo con il valore esatto di Onsager ($T_c = 2.269185...$), con un errore relativo dello 0.016\%.

\subsection{Energia e Calore Specifico}

L'energia per spin e il calore specifico forniscono informazioni complementari sulla transizione di fase.

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{../plots/energy_heat.png}
    \caption{Energia per spin e calore specifico vs temperatura. (a) L'energia mostra una transizione continua ma ripida a $T_c$. (b) Il calore specifico presenta un picco marcato alla transizione.}
    \label{fig:EC}
\end{figure}

\paragraph{Energia per spin:}
La Figura~\ref{fig:EC}(a) mostra l'andamento di $E(T)$. Osserviamo (anche se i limiti non sono ben visibili a causa del range di temperature limitato):
\begin{itemize}
    \item \textbf{Fase ferromagnetica} ($T < T_c$): $E \to -2$ (energia minima: tutti gli spin allineati, ogni spin ha 4 vicini con orientazione uguale, contribuendo $-4 \times 1/2 = -2$ per spin).
    \item \textbf{Fase paramagnetica} ($T > T_c$): $E \to 0$ (spin disordinati, contributi positivi e negativi si bilanciano in media).
    \item \textbf{Transizione continua}: A differenza di una transizione del primo ordine, non c'è discontinuità (salto) nell'energia. La transizione è \emph{continua} ma con derivata divergente.
\end{itemize}

La pendenza crescente di $E(T)$ vicino a $T_c$ segnala l'aumento delle fluttuazioni termiche, misurate dal calore specifico.

\paragraph{Calore specifico:}
Il calore specifico $C = \frac{\partial E}{\partial T} = \beta^2 N (\langle E^2 \rangle - \langle E \rangle^2)$ quantifica la capacità del sistema di assorbire energia termica. La Figura~\ref{fig:EC}(b) mostra un picco pronunciato a $T_c$.

Per il modello di Ising 2D, la teoria di Onsager predice una \textbf{divergenza logaritmica}:
\begin{equation}
    C(T) \sim -\ln|T - T_c|
\end{equation}

Caratteristiche osservate:
\begin{itemize}
    \item \textbf{Picco a $T_c$}: Il massimo indica la temperatura dove le fluttuazioni di energia sono massime, corrispondente alla transizione di fase.
    \item \textbf{Altezza crescente con $L$}: Su sistemi finiti, il picco cresce con $L$ ma rimane finito. Nel limite termodinamico ($L \to \infty$), diverge logaritmicamente.
    \item \textbf{Stima di $T_c$ dai picchi}: Analogamente alla suscettività, la posizione dei picchi si sposta con $L$ secondo:
    \begin{equation}
        T_{\max}^{(C)}(L) = T_c + b \cdot L^{-1/\nu}
    \end{equation}
    Fittando questa relazione FSS ai dati si ottiene:
    \begin{equation}
        T_c^{(C)} = 2.270 \pm 0.002, \quad b = 0.77 \pm 0.09
    \end{equation}
    Questo valore è in buon accordo con il valore esatto di Onsager (errore $\sim 0.04\%$) e con il valore ottenuto dal Binder crossing. Il metodo FSS è significativamente più accurato della semplice media dei picchi, che darebbe $T_c \approx 2.282 \pm 0.011$.
\end{itemize}

Il calore specifico è sensibile agli effetti di finite-size e meno adatto della suscettività per determinare $T_c$ tramite scaling. Tuttavia, conferma quantitativamente la posizione della transizione di fase e illustra la natura critica del sistema al punto di transizione.

\section{Conclusioni}

È stato studiato numericamente la transizione di fase del modello di Ising 2D con l'algoritmo di Wolff. I risultati principali:

\begin{enumerate}
    \item \textbf{Temperatura critica}:
    \begin{itemize}
        \item Binder crossing: $T_c = 2.269 \pm 0.006$ (errore $\sim 0.02\%$)
        \item FSS picchi $\chi$: $T_c = 2.270 \pm 0.001$ (errore $\sim 0.04\%$)
        \item FSS picchi $C$: $T_c = 2.270 \pm 0.002$ (errore $\sim 0.04\%$)
        \item Valore esatto (Onsager): $T_c = 2.269185...$
    \end{itemize}
    I tre metodi danno risultati in eccellente accordo tra loro e con il valore teorico. Il metodo del Binder crossing, pur avendo un errore statistico leggermente maggiore, è il più robusto poiché non richiede assunzioni sulla forma funzionale dello shift dei picchi.
    \item \textbf{Esponente critico (scaling $\chi$)}: $\gamma/\nu = 1.739 \pm 0.004$ (esatto: $1.75$). La differenza di $\sim 0.6\%$ ($\sim 2.75\sigma$) suggerisce la presenza di piccole correzioni finite-size non catturate dal semplice power-law scaling.
    \item \textbf{Finite Size Scaling}: data collapse validato con $R^2 > 0.999$
\end{enumerate}

L'eccellente accordo con i valori esatti dimostra:
\begin{itemize}
    \item Efficacia dell'algoritmo di Wolff
    \item Validità della teoria FSS
    \item Accuratezza dei metodi Monte Carlo
\end{itemize}

L'algoritmo di Wolff supera il critical slowing down, permettendo simulazioni efficienti anche per $L$ grandi.

\end{multicols}

\end{document}
